{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "# 4. Categorical iMM + Gibbs Sampling Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "# Common imports \n",
    "from ast import literal_eval\n",
    "\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math \n",
    "\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## 1. Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "I = 120 # Number of words in the dictionary\n",
    "N = None # Number of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and preprocess the data\n",
    "\n",
    "First, we need to load the data from the csv. This file contains the documents already processed and cleaned after applying the following steps:\n",
    "\n",
    "1. Tokenization\n",
    "2. Homogeneization, which includes:\n",
    "    1. Removing capitalization.\n",
    "    2. Removing non alphanumeric tokens (e.g. punktuation signs)\n",
    "    3. Stemming/Lemmatization.\n",
    "3. Cleaning\n",
    "4. Vectorization\n",
    "\n",
    "\n",
    "We load it as a `pandas` dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('tweets_cleaned.csv')\n",
    "df.drop_duplicates(subset=\"tweet\", inplace=True)\n",
    "\n",
    "df['tokens'] = df['tokens'].apply(literal_eval) #Transform the string into a list of tokens\n",
    "X_tokens = list(df['tokens'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: tweet_id | timestamp | user_id | tweet | tweets_clean | tokens\n",
      "\n",
      "Tweet:\n",
      "OSINT people - please retweet, if possible. My friend is looking for women involved in OSINT. https://twitter.com/manisha_bot/status/1181594280336531457 …\n",
      "Tweet cleaned:\n",
      "osint people   please retweet  if possible  my friend is looking for women involved in osint\n",
      "Tweet tokens:\n",
      "['osint', 'peopl', 'retweet', 'possibl', 'friend', 'look', 'woman', 'involv', 'osint']\n"
     ]
    }
   ],
   "source": [
    "print('Columns: {}\\n'.format(' | '.join(df.columns.values)))\n",
    "\n",
    "print('Tweet:\\n{}'.format(df.loc[1, 'tweet']))\n",
    "print('Tweet cleaned:\\n{}'.format(df.loc[1, 'tweets_clean']))\n",
    "print('Tweet tokens:\\n{}'.format(X_tokens[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dictionary\n",
    "\n",
    "Up to this point, we have transformed the raw text collection in a list of documents stored in `X_tokens`, where each document is a collection \n",
    "of the words that are most relevant for semantic analysis. Now, we need to convert these data (a list of token lists) into \n",
    "a numerical representation (a list of vectors, or a matrix). To do so, we will start using the tools provided by the `gensim` library. \n",
    "\n",
    "As a first step, we create a dictionary containing all tokens in our text corpus, and assigning an integer identifier to each one of them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(X_tokens)\n",
    "\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=I)\n",
    "\n",
    "I = len(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Bag of Words (BoW): Numerical version of documents\n",
    "In the second step, let us create a numerical version of our corpus using the `doc2bow` method. In general, \n",
    "`D.doc2bow(token_list)` transforms any list of tokens into a list of tuples `(token_id, n)`, one per each token in \n",
    "`token_list`, where `token_id` is the token identifier (according to dictionary `D`) and `n` is the number of occurrences \n",
    "of such token in `token_list`. \n",
    "\n",
    "*Exercise:* Apply the `doc2bow` method from gensim dictionary `D`, to all tokens in every document in `X_tokens`. \n",
    "The result must be a new list named `X_bow` where each element is a list of tuples `(token_id, number_of_occurrences)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_bow = list()\n",
    "keep_tweet = list()\n",
    "for tweet in X_tokens:\n",
    "    tweet_bow = dictionary.doc2bow(tweet)\n",
    "    if len(tweet_bow) > 1:\n",
    "        X_bow.append(tweet_bow)\n",
    "        keep_tweet.append(True)\n",
    "    else:\n",
    "        keep_tweet.append(False)\n",
    "\n",
    "df_data = df[keep_tweet]\n",
    "\n",
    "N = len(df_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we transform the BoW representation `X_bow` into a matrix, namely `X_matrix`, in which the i-th row and j-th column represents the \n",
    "number of occurrences of the j-th word of the dictionary in the i-th document. This will be the matrix used in the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: 5568 I: 120\n"
     ]
    }
   ],
   "source": [
    "X_matrix = np.zeros([N, I])\n",
    "for i, doc_bow in enumerate(X_bow):\n",
    "    word_list = list()\n",
    "    for word in doc_bow:\n",
    "        X_matrix[i, word[0]] = word[1]\n",
    "\n",
    "        \n",
    "print('N: {} I: {}'.format(N, I))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1\n",
    "Derive the posterior $p( \\boldsymbol{Z}, \\boldsymbol{\\Theta}| \\boldsymbol{X})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2\n",
    "Derive the posterior $p(\\boldsymbol{Z}| \\boldsymbol{X})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "Implement the three algorithms in the boxes below (or in a separated python file that can be imported)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Algorithm 1 named gibbs_2 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def log_likelihood_zn_theta(X_matrix, Thetas, K):\n",
    "    \n",
    "    \"\"\"\n",
    "    Computes the log-likelihood logP(x_n | z_n = k, Thetas[k]) for\n",
    "    every document x_n and every cluster k.\n",
    "    \n",
    "    Arguments:\n",
    "        X_matrix: np.ndarray, (N x I), X_matrix[i, j] represents the \n",
    "            number of occurences of the j-th word of the dictionary in \n",
    "            the i-th document.\n",
    "        Thetas: np.ndarray, (K x I), likelihood parameters for all cluster\n",
    "    \n",
    "    Returns:\n",
    "        log_likelihoods: np.ndarray, (N x K), log_likelihoods[i, j] represents\n",
    "            logP(x_i | z_i = j, Thetas[j])\n",
    "    \"\"\"\n",
    "    N = X_matrix.shape[0]\n",
    "    \n",
    "    Thetas = np.array(Thetas).reshape(K,I)\n",
    "    \n",
    "    log_likelihoods = np.ones((N, K))\n",
    "    \n",
    "    \n",
    "    for n, document in enumerate(X_matrix):\n",
    "        \n",
    "        c_n = np.repeat(document[np.newaxis, :], K, axis=0) # K x I\n",
    "        log_likelihoods[n, :] = (c_n * np.log(Thetas)).sum(axis=1) # K\n",
    "    \n",
    "    return log_likelihoods #N x K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_marginal_likelihood_zn_theta(Xn, gamma):\n",
    "    \n",
    "    \"\"\"\n",
    "    Computes the marginal log-likelihood logP(x_n, Theta | z_n = k) for\n",
    "    one document x_n and every cluster k over all possible values of Theta.\n",
    "    \n",
    "    Arguments:\n",
    "        Xn: np.ndarray, (1 x I), Xn[j] represents the \n",
    "            number of occurences of the j-th word of the dictionary in \n",
    "            the document.\n",
    "        gamma: gamma hyperparameter\n",
    "    \n",
    "    Returns:\n",
    "        integral log_likelihoods: np.ndarray, (1 x K), log_marginal_likelihood[j] represents\n",
    "            logP(x_n | z_n = j)\n",
    "    \"\"\"\n",
    "\n",
    "    gamma = gamma + Xn\n",
    "    \n",
    "    # compute sum_m sum_i log(gamma_m + i)\n",
    "    summands_i = np.copy(Xn)\n",
    "    mask = (Xn > 0)\n",
    "    summands_i -= (summands_i > 0)\n",
    "    log_s_m = mask*np.log(gamma + summands_i) # N x I\n",
    "    \n",
    "    while np.any(summands_i > 0):\n",
    "        summands_i -= np.ones(Xn.shape)\n",
    "        mask[summands_i < 0] = 0\n",
    "        summands_i[summands_i < 0] = 0\n",
    "            \n",
    "        log_s_m += mask * np.log(gamma + summands_i) # N x I\n",
    "    log_s_m = np.sum(log_s_m, axis=1) # N\n",
    "              \n",
    "    # compute sum_j log(j + sum_m gamma_m)\n",
    "    summands_j = np.copy(Xn).sum(axis=1) # N\n",
    "    mask = (summands_j > 0)\n",
    "    summands_j -= (summands_j > 0)\n",
    "    gamma_sum = gamma.sum(axis=1) # N\n",
    "    log_w_n = mask*np.log(summands_j + gamma_sum)\n",
    "    \n",
    "    while np.any(summands_j > 0):\n",
    "        summands_j -= np.ones(1)#(summands_j > 0)\n",
    "        mask[summands_j < 0] = 0\n",
    "        summands_j[summands_j < 0] = 0\n",
    "            \n",
    "        log_w_n += mask * np.log(summands_j + gamma_sum)\n",
    "        \n",
    "    # compute log_marginal-likelihoods\n",
    "    log_marginal_likelihood = log_s_m - log_w_n\n",
    "        \n",
    "        \n",
    "    return log_marginal_likelihood\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_posterior_z_minus_n_for_known_clusters(Z, alpha, K, zn):\n",
    "    \"\"\"\n",
    "    Computes the log-posterior logP(z_n = k | Z_minus_n) for all\n",
    "    n and all k.\n",
    "    \n",
    "    Arguments:\n",
    "        Z: np.ndarray (N), contains the cluster assignment of\n",
    "            the documents\n",
    "        alpha: np.ndarray (K), parameter for the prior distribution\n",
    "            of Pi\n",
    "        cluster_counts: np.ndarray (K), cluster_counts[i] is the number of\n",
    "            documents in the i-th cluster\n",
    "    Returns:\n",
    "        log_posteriors: np.ndarray, (N x K), log_posteriors[i,j] represents\n",
    "            logP(z_i = j | Z_minus_i)\n",
    "    \"\"\"\n",
    "    \n",
    "    N = Z.shape[0]\n",
    "    \n",
    "    cluster_counts = np.zeros(K, dtype=np.int)\n",
    "    cluster, counts = np.unique(Z, return_counts=True)  \n",
    "    cluster_counts[cluster] = counts  \n",
    "    cluster_counts[zn] -=1\n",
    "    \n",
    "    log_posterior_z_minus_n = np.log(cluster_counts) - np.log( N - 1 + (alpha.sum()) )\n",
    "        \n",
    "    return  log_posterior_z_minus_n   #(1 x K)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_posterior_z_minus_n_for_new_cluster(X_matrix, Z, alpha):\n",
    "    \"\"\"\n",
    "        Computes the logposterior P(z_n = k | Z_minus_n) for Xn and all k\n",
    "        Arguments:\n",
    "        Z: np.ndarray (N), contains the cluster assignment of the documents\n",
    "        alpha: np.ndarray (K), parameter for the prior distribution of Pi\n",
    "        cluster_counts: np.ndarray (K), cluster_counts[i] is the number of\n",
    "        documents in the i-th cluster\n",
    "    Returns:\n",
    "        posteriors: np.ndarray, (1 x 1), posteriors[i] represents\n",
    "            P(z_n = i | Z_minus_i)\n",
    "    \"\"\"\n",
    "    R = X_matrix.shape[0]   \n",
    "    log_posterior_z_minus_n =  np.log(alpha.sum()) - np.log( N - 1 + (alpha.sum()) ) \n",
    "    log_posterior_z_minus_n = np.array([log_posterior_z_minus_n])\n",
    "    \n",
    "    log_posterior_z_minus_n = np.repeat(log_posterior_z_minus_n[np.newaxis, :], R, axis=0) # (R x 1)   \n",
    "    \n",
    "    return  log_posterior_z_minus_n #(R x 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_theta_k(X_matrix, Z, gamma, k):\n",
    "    \"\"\"\n",
    "    Samples cluster parameters Theta_k from the posterior P(Theta_k | X, Z).\n",
    "    \n",
    "    Arguments:\n",
    "        X_matrix: np.ndarray (N x I), X_matrix[i, j] represents the \n",
    "            number of occurences of the j-th word of the dictionary in \n",
    "            the i-th document.\n",
    "        Z: np.ndarray (N), contains the cluster assignment of\n",
    "            the documents\n",
    "        gamma: np.ndarray (I), parameter for the prior distribution\n",
    "            of Theta\n",
    "        k: integer, cluster id\n",
    "    \n",
    "    Returns:\n",
    "        Theta_k: np.ndarray (I), contains the parameters of the k-th\n",
    "            cluster\n",
    "    \"\"\"\n",
    "    # compute the number of occurances for each word in cluster k\n",
    "    word_cluster_counts = X_matrix[np.where(Z == k), :].sum(axis=1)[0]\n",
    "    gamma_new = gamma + word_cluster_counts\n",
    "\n",
    "    return np.random.dirichlet(gamma_new, size=1)[0] #(1 x I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_theta_new(X_matrix, n, gamma):\n",
    "    \"\"\"\n",
    "    Samples cluster parameters Theta_k_new from the posterior P(Theta_k_new | Xn).\n",
    "    \n",
    "    Arguments:\n",
    "        X_matrix: np.ndarray (N x I), X_matrix[i, j] represents the \n",
    "            number of occurences of the j-th word of the dictionary in \n",
    "            the i-th document.\n",
    "        gamma: np.ndarray (I), parameter for the prior distribution\n",
    "            of Theta\n",
    "    \n",
    "    Returns:\n",
    "        Theta_k_new: np.ndarray (I), contains the parameters of the new\n",
    "            cluster\n",
    "    \"\"\"\n",
    "    # compute the number of occurances for each word in cluster k_new (i.e number of words in doc Xn)  \n",
    "    word_cluster_counts = X_matrix[n]\n",
    "    gamma_new = gamma + word_cluster_counts\n",
    "\n",
    "    return np.random.dirichlet(gamma_new, size=1)[0]  #(1 x I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sum_exp(summands, axis=0):\n",
    "    \n",
    "    \"\"\"\n",
    "    Computes log(sum_i exp(summands[i])), using the log-sum-exp trick.\n",
    "    \n",
    "    Arguments:\n",
    "        summands: np.ndarray, contains the summands\n",
    "        axis: integer, the axis to sum over\n",
    "        \n",
    "    Returns:\n",
    "        log_sum_exp: np.ndarray, the results of the summation\n",
    "    \"\"\"\n",
    "    \n",
    "    max_summand = np.max(summands)\n",
    "    log_sum_exp = max_summand + np.log(np.exp(summands - max_summand).sum(axis=axis))\n",
    "    \n",
    "    return log_sum_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_normalize(log_dist):\n",
    "    \"\"\"\n",
    "    Computes exp(log_dist) / sum_i(log_dist[i]) using the exp-normalize trick.\n",
    "    \n",
    "    Arguments:\n",
    "        log_dist: np.ndarray, (N x K), log-distribution that will be normalized\n",
    "        \n",
    "    Returns:\n",
    "        norm_dist: np.ndarray, (log_dist.shape), the normalized distribution\n",
    "    \"\"\"\n",
    "    K = log_dist.shape[1]\n",
    "    \n",
    "    max_val = np.repeat(np.max(log_dist, axis=1)[:, np.newaxis], K, axis=1) # (N x K)\n",
    "    \n",
    "    norm_dist = np.exp(log_dist - max_val) # (N x K)\n",
    "\n",
    "    sum_exp = norm_dist.sum(axis=1) # (N)\n",
    "    norm_dist /= np.repeat(sum_exp[:, np.newaxis], K, axis=1)\n",
    "\n",
    "    return norm_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_zn_2(X_matrix, n, Thetas, K, gamma, alpha, Z):\n",
    "    \"\"\"\n",
    "    Computes the posterior P(z_n = k| x_n, Z_minus_n, Theta) for all\n",
    "    n and all k.\n",
    "    \n",
    "    Arguments:\n",
    "        log_lik_zn_theta: np.ndarray, (N x K), log_lik_zn_theta[i, j] represents\n",
    "            logP(x_i | z_i = j, Theta)\n",
    "        log_post_z_minus_n: np.ndarray, (N x K), log_post_z_minus_n[i, j] represents\n",
    "            logP(z_i = j | Z_minus_j)\n",
    "        \n",
    "    Returns:\n",
    "        posteriors: np.ndarray, (N x K), posteriors[i, j] represents\n",
    "            P(z_i = j | x_i, Z_minus_i, Theta)\n",
    "    \"\"\"\n",
    "    \n",
    "    dist1 =  log_likelihood_zn_theta(X_matrix[n][np.newaxis, :], Thetas, K) + log_posterior_z_minus_n_for_known_clusters(Z, alpha, K, Z[n])                                \n",
    "    \n",
    "    dist2 = log_marginal_likelihood_zn_theta(X_matrix[n][np.newaxis, :], gamma) + log_posterior_z_minus_n_for_new_cluster(X_matrix[n][np.newaxis, :], Z, alpha)    \n",
    "    \n",
    "    posterior_zn = np.zeros((1,K+1))\n",
    "    posterior_zn[:,:-1] = dist1\n",
    "    posterior_zn[0][K] = dist2\n",
    "    \n",
    "\n",
    "    return exp_normalize(posterior_zn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_z_2(post_zn_2):\n",
    "    \"\"\"\n",
    "    Samples the cluster assignment z_n of x_n from the posterior \n",
    "    P(z_n | x_n, Z_-n, Theta).\n",
    "\n",
    "    Arguments:\n",
    "        post_zn: np.ndarray, (K), post_zn[i] represents\n",
    "            p(z_n = i | x_n, Pi, Theta)\n",
    "    Returns:\n",
    "        z_n: integer, sampled cluster assignment of x_n\n",
    "    \"\"\"\n",
    "    L = len(post_zn_2)\n",
    "    return np.random.choice(np.arange(L), p=post_zn_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(log_lik_zn_theta, Z):\n",
    "\n",
    "    \n",
    "    summands = log_lik_zn_theta # (N x K)\n",
    "    summands = summands[np.arange(N), Z] # (N)\n",
    "    return summands.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_plot(ax, h):\n",
    "    \"\"\"\n",
    "    Prepares the animated log-likelihood plot\n",
    "    \"\"\"\n",
    "    ax[h].set_xlabel(\"Steps\", fontweight='bold', fontsize=15)\n",
    "    ax[h].set_ylabel(\"Log-likelihood\", fontweight='bold', fontsize=15)\n",
    "    line, = ax[h].plot([0], [0])\n",
    "    \n",
    "    lik = []\n",
    "    \n",
    "    return lik, line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gibbs_2(X_matrix, alpha_value, fig, ax, h):\n",
    "    \n",
    "    # prepare likelihood plot animation\n",
    "    lik_2, line = prepare_plot(ax, h)\n",
    "        \n",
    "\n",
    "    def animate(i):\n",
    "        line.axes.set_xlim(0, i)\n",
    "        line.axes.set_ylim(0.9 * np.min(lik_2), 1.1 * np.max(lik_2))\n",
    "        line.axes.invert_yaxis()\n",
    "        line.set_data(np.arange(i), lik_2[:i])         \n",
    "            \n",
    "    np.random.seed(1234)\n",
    "    \n",
    "\n",
    "    N, I = X_matrix.shape   \n",
    "    \n",
    "    gamma = np.ones(I)\n",
    "    # sample Theta from prior\n",
    "    Thetas = np.array( np.random.dirichlet(gamma, size=1)[0] )\n",
    "    K = 1\n",
    "    # sample Z from prior\n",
    "    Z = np.repeat(0,N) # 1 x N\n",
    "    # create alpha \n",
    "    alpha = (np.ones(K) / K) * alpha_value\n",
    "    \n",
    "    step = 1\n",
    "    while(True):\n",
    "        \n",
    "        for n in range(N):\n",
    "            posterior_zn = posterior_zn_2(X_matrix, n, Thetas, K, gamma, alpha, Z)\n",
    "            Z[n] = sample_z_2(posterior_zn[0])\n",
    "        \n",
    "            if Z[n] == K : \n",
    "                theta = sample_theta_new(X_matrix, n, gamma)\n",
    "                Thetas = np.vstack((Thetas, theta))\n",
    "                K+=1\n",
    "                alpha = (np.ones(K) / K) * alpha_value\n",
    "    \n",
    "        for k in range(K):\n",
    "            Thetas[k] = sample_theta_k(X_matrix, Z, gamma, k)\n",
    "        step +=1\n",
    "        log_lik_zn_theta = log_likelihood_zn_theta(X_matrix, Thetas, K)       \n",
    "        lik_2.append(log_likelihood(log_lik_zn_theta, Z))\n",
    "        if (step) % 9 == 0:\n",
    "            aux_list = lik_2[len(lik_2)-10 : len(lik_2)-1]\n",
    "            if len(aux_list) == aux_list.count(aux_list[len(aux_list)-1]) : break \n",
    "            \n",
    "        \n",
    "        \n",
    "        # plot likelihood\n",
    "        animate(step-1)\n",
    "        clear_output(wait=True)\n",
    "        display(fig)\n",
    "        plt.show()\n",
    "        print(f\"Iteration: {(step)} \\t Log-likelihood: {lik_2[step-2]}\")\n",
    "    \n",
    "    # plot likelihood\n",
    "    animate(step-2)\n",
    "    clear_output(wait=True)\n",
    "    display(fig)\n",
    "    print(f\"Iteration: {(step)} \\t Log-likelihood: {lik_2[step-2]}\")\n",
    "\n",
    "    return Z, lik_2, Thetas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2 For the First algorithm (gibbs_2):\n",
    "Some useful packages:\n",
    "- matplotlib https://matplotlib.org/\n",
    "- seaborn https://github.com/mwaskom/seaborn\n",
    "- wordcloud https://github.com/amueller/word_cloud\n",
    "- probvis https://github.com/psanch21/prob-visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we run the Algorithm until convergence and plot the evolution of the likelihood per iteration (question a):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7 \t Log-likelihood: -91128.17284978065\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10,7))\n",
    "\n",
    "Thetas_result = []\n",
    "K_plus = []\n",
    "alpha_values = [ 0.1 , 10]\n",
    "h = 0\n",
    "for alpha_value in alpha_values:\n",
    "    Z, lik_2, Thetas = gibbs_2(X_matrix, alpha_value, fig, ax, h)\n",
    "    h+=1\n",
    "    \n",
    "    cluster_counts = np.zeros(Thetas.shape [0], dtype=np.int)\n",
    "    cluster, counts = np.unique(Z, return_counts=True)  \n",
    "    cluster_counts[cluster] = counts \n",
    "    indices = np.where(cluster_counts == 0)[0]\n",
    "    X = np.ma.masked_equal(cluster_counts,0)\n",
    "    X = X.compressed()\n",
    "    Thetas = np.delete(Thetas, indices, 0)\n",
    "    \n",
    "    K_plus.append(X.size)\n",
    "    Thetas_result.append(Thetas)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we plot the 10 most representative words from each topic from a sample (question b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "tweet_array = np.array(df_data['tweet'].values)\n",
    "for thetas in Thetas_result :\n",
    "    P = 4\n",
    "    ToPlot = [thetas[n:n+P] for n in range(0, thetas.shape[0], P)]\n",
    "    print('This is a collection of Plots for the alpha value :', alpha_values[Thetas_result.index(thetas)])\n",
    "    print()\n",
    "    for e in ToPlot:\n",
    "        \n",
    "        fig, ax = plt.subplots( 1, 4, figsize=(100,100))\n",
    "        for k in range(e.shape[0]):\n",
    "            \n",
    "            theta_k = e[k]\n",
    "            # get indices of 10 most representative words\n",
    "            ind = np.argsort(-theta_k)[:10]\n",
    "    \n",
    "            words = [dictionary[i] for i in ind]     \n",
    "            frequencies = {word:freq for word, freq in zip(words, theta_k[ind])}\n",
    "        \n",
    "\n",
    "            # generate wordcloud\n",
    "            wordcloud = WordCloud().generate_from_frequencies(frequencies)\n",
    "        \n",
    "           \n",
    "            ax[k].imshow(wordcloud, interpolation='bilinear')\n",
    "            ax[k].axis(\"off\")\n",
    "        \n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we plot the alpha vs. number of clusters for the Algorithm (question c): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(alpha_values, K_plus)\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"number of clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Algorithm 2 named gibbs_3 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_zn_Xmn_Zmn(X_matrix, Xn, n,  Z, gamma, K):\n",
    "    \"\"\"\n",
    "    Computes the log-likelihood logP(x_n | z_n = k, X_-n, Z_-n) for Xn.\n",
    "    \n",
    "    Arguments:\n",
    "        X_matrix: np.ndarray, (N x I), X_matrix[i, j] represents the \n",
    "            number of occurences of the j-th word of the dictionary in \n",
    "            the i-th document.\n",
    "        Xn : the current element (1,N)\n",
    "        n : indice of the current element\n",
    "        Z: np.ndarray (N), contains the cluster assignments\n",
    "        gamma: np.ndarray (I), parameter for the prior distribution\n",
    "            of Theta\n",
    "        K: integer, number of clusters \n",
    "        \n",
    "    Returns:\n",
    "        log_likelihoods: np.ndarray, (1 x K), log_likelihoods[j]\n",
    "            represents logP(x_n| z_i = j, X_-n, Z_-n) \n",
    "    \"\"\"\n",
    "    N, I = X_matrix.shape\n",
    "    log_likelihoods = np.zeros((1, K))\n",
    "    \n",
    "    \n",
    "    for k in range(K):\n",
    "        \n",
    "        # compute gamma_dash\n",
    "        X_k = X_matrix[(Z == k), :]\n",
    "        gamma_dash = X_k.sum(axis=0) # 1 x I \n",
    "        #if Z[n] == k : gamma_dash[k] -= 1\n",
    "        gamma_dash = gamma_dash + gamma\n",
    "\n",
    "        \n",
    "        # compute sum_m sum_i log(gamma_m + i)\n",
    "        summands_i = np.copy(Xn)\n",
    "        mask = (Xn > 0)\n",
    "        summands_i -= (summands_i > 0)\n",
    "        log_s_m = mask*np.log(gamma_dash + summands_i    +   1e-7) # 1 x I\n",
    "    \n",
    "        while np.any(summands_i > 0):\n",
    "            summands_i -= np.ones(Xn.shape)\n",
    "            mask[summands_i < 0] = 0\n",
    "            summands_i[summands_i < 0] = 0\n",
    "            \n",
    "            log_s_m += mask * np.log(gamma_dash + summands_i) # 1 x I\n",
    "        log_s_m = np.sum(log_s_m, axis=1) # 1\n",
    "        \n",
    "        \n",
    "        # compute sum_j log(j + sum_m gamma_m)\n",
    "        summands_j = np.copy(Xn).sum(axis=1) # 1\n",
    "        \n",
    "        mask = (summands_j > 0)\n",
    "        summands_j -= (summands_j > 0)\n",
    "        gamma_sum = gamma_dash.sum(axis=0) # 1\n",
    "        log_w_n = mask*np.log(summands_j + gamma_sum    +  1e-7)\n",
    "        \n",
    "        while np.any(summands_j > 0):\n",
    "            summands_j -= np.ones(1)#(summands_j > 0)\n",
    "            mask[summands_j < 0] = 0\n",
    "            summands_j[summands_j < 0] = 0\n",
    "            \n",
    "            log_w_n += mask * np.log(summands_j + gamma_sum)\n",
    "            \n",
    "        # compute log-likelihoods\n",
    "        log_likelihoods[:,k] = log_s_m - log_w_n\n",
    "        \n",
    "    return log_likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_zn_3(X_matrix, n, K, gamma, alpha, Z):\n",
    "    \"\"\"\n",
    "    Computes the posterior P(z_n = k| x_n, Z_minus_n, Theta) for all\n",
    "    n and all k.\n",
    "    \n",
    "    Arguments:\n",
    "        log_lik_zn_theta: np.ndarray, (N x K), log_lik_zn_theta[i, j] represents\n",
    "            logP(x_i | z_i = j, Theta)\n",
    "        log_post_z_minus_n: np.ndarray, (N x K), log_post_z_minus_n[i, j] represents\n",
    "            logP(z_i = j | Z_minus_j)\n",
    "        \n",
    "    Returns:\n",
    "        posteriors: np.ndarray, (N x K), posteriors[i, j] represents\n",
    "            P(z_i = j | x_i, Z_minus_i, Theta)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    dist1 = log_likelihood_zn_Xmn_Zmn(X_matrix, X_matrix[n][np.newaxis, :], n, Z, gamma, K) + log_posterior_z_minus_n_for_known_clusters(Z, alpha, K, Z[n])\n",
    "\n",
    "    \n",
    "    dist2 =  log_marginal_likelihood_zn_theta(X_matrix[n][np.newaxis, :], gamma) +  log_posterior_z_minus_n_for_new_cluster(X_matrix[n][np.newaxis, :], Z, alpha)                                  \n",
    "    \n",
    "    \n",
    "    \n",
    "    posterior_zn = np.zeros((1,K+1))\n",
    "    posterior_zn[:,:-1] = dist1\n",
    "    posterior_zn[0][K] = dist2\n",
    "    \n",
    "    \n",
    "\n",
    "    return exp_normalize(posterior_zn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_z_3(post_zn_3):\n",
    "    \"\"\"\n",
    "    Samples the cluster assignment z_n of x_n from the posterior \n",
    "    P(z_n | x_n, Z_-n, Theta).\n",
    "\n",
    "    Arguments:\n",
    "        post_zn: np.ndarray, (K), post_zn[i] represents\n",
    "            p(z_n = i | x_n, Pi, Theta)\n",
    "    Returns:\n",
    "        z_n: integer, sampled cluster assignment of x_n\n",
    "    \"\"\"\n",
    "    L = len(post_zn_3)\n",
    "    return np.random.choice(np.arange(L), p=post_zn_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs_3(X_matrix, alpha_value, fig, ax, h):\n",
    "    \n",
    "    \n",
    "    # prepare likelihood plot animation\n",
    "    lik_3, line = prepare_plot(ax, h)\n",
    "    \n",
    "    \n",
    "    def animate(i):\n",
    "        line.axes.set_xlim(0, i)\n",
    "        line.axes.set_ylim(0.9 * np.min(lik_3), 1.1 * np.max(lik_3))\n",
    "        line.axes.invert_yaxis()\n",
    "        line.set_data(np.arange(i), lik_3[:i])\n",
    "            \n",
    "            \n",
    "    np.random.seed(1234)\n",
    "    \n",
    "\n",
    "    N, I = X_matrix.shape   \n",
    "    \n",
    "    gamma = np.ones(I)\n",
    "    # sample Theta from prior\n",
    "    Thetas = np.array( np.random.dirichlet(gamma, size=1)[0] )\n",
    "    K = 1\n",
    "    # sample Z from prior\n",
    "    Z = np.repeat(0,N) # 1 x N\n",
    "    # create alpha \n",
    "    alpha = (np.ones(K) / K) * alpha_value\n",
    "    \n",
    "    step = 1\n",
    "    while(True):\n",
    "        \n",
    "        for n in range(N):\n",
    "            posterior_zn = posterior_zn_3(X_matrix, n, K, gamma, alpha, Z)\n",
    "            Z[n] = sample_z_3(posterior_zn[0])\n",
    "        \n",
    "            if Z[n] == K : \n",
    "                K+=1\n",
    "                alpha = (np.ones(K) / K) * alpha_value\n",
    "                 \n",
    "    \n",
    "        Thetas = np.zeros((K, I))       \n",
    "        for k in range(K):\n",
    "            # sample likelihood parameters Theta_k\n",
    "            Thetas[k] = sample_theta_k(X_matrix, Z, gamma, k) # I\n",
    "            \n",
    "        log_lik_zn_theta = log_likelihood_zn_theta(X_matrix, Thetas, K)\n",
    "        log_lik = log_likelihood(log_lik_zn_theta, Z)\n",
    "        lik_3.append(log_lik)\n",
    "        \n",
    "        step +=1\n",
    "        print(step)\n",
    "        if (step) % 9 == 0:\n",
    "            aux_list = lik_3[len(lik_3)-10 : len(lik_3)-1]\n",
    "            if len(aux_list) == aux_list.count(aux_list[len(aux_list)-1]) : break \n",
    "        \n",
    "\n",
    "        animate(step-1)\n",
    "        clear_output(wait=True)\n",
    "        display(fig)\n",
    "        plt.show()\n",
    "        print(f\"Iteration: {(step)} \\t Log-likelihood: {lik_3[step-2]}\")\n",
    "    \n",
    "    # plot likelihood\n",
    "    animate(step-2)\n",
    "    clear_output(wait=True)\n",
    "    display(fig)\n",
    "    plt.show()\n",
    "    print(f\"Iteration: {(step)} \\t Log-likelihood: {lik_3[step-2]}\")\n",
    "\n",
    "\n",
    "    return Z, lik_3, Thetas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2 For the Second algorithm (gibbs_3):\n",
    "Some useful packages:\n",
    "- matplotlib https://matplotlib.org/\n",
    "- seaborn https://github.com/mwaskom/seaborn\n",
    "- wordcloud https://github.com/amueller/word_cloud\n",
    "- probvis https://github.com/psanch21/prob-visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we run the Algorithm until convergence and plot the evolution of the likelihood per iteration (question a):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10,7))\n",
    "\n",
    "Thetas_result = []\n",
    "K_plus = []\n",
    "alpha_values = [ 0.1 , 10 ]\n",
    "h = 0\n",
    "for alpha_value in alpha_values:\n",
    "    Z, lik_3, Thetas = gibbs_3(X_matrix, alpha_value, fig, ax, h)\n",
    "    h+=1\n",
    "    \n",
    "    cluster_counts = np.zeros(Thetas.shape [0], dtype=np.int)\n",
    "    cluster, counts = np.unique(Z, return_counts=True)  \n",
    "    cluster_counts[cluster] = counts \n",
    "    indices = np.where(cluster_counts == 0)[0]\n",
    "    X = np.ma.masked_equal(cluster_counts,0)\n",
    "    X = X.compressed()\n",
    "    Thetas = np.delete(Thetas, indices, 0)\n",
    "    \n",
    "    K_plus.append(X.size)\n",
    "    Thetas_result.append(Thetas) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we plot the 10 most representative words from each topic from a sample (question b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "tweet_array = np.array(df_data['tweet'].values)\n",
    "for thetas in Thetas_result :\n",
    "    P = 4\n",
    "    ToPlot = [thetas[n:n+P] for n in range(0, thetas.shape[0], P)]\n",
    "    print('This is a collection of Plots for the alpha value :', alpha_values[Thetas_result.index(thetas)])\n",
    "    print()\n",
    "    for e in ToPlot:\n",
    "        \n",
    "        fig, ax = plt.subplots( 1, 4, figsize=(100,100))\n",
    "        for k in range(e.shape[0]):\n",
    "            \n",
    "            theta_k = e[k]\n",
    "            # get indices of 10 most representative words\n",
    "            ind = np.argsort(-theta_k)[:10]\n",
    "    \n",
    "            words = [dictionary[i] for i in ind]     \n",
    "            frequencies = {word:freq for word, freq in zip(words, theta_k[ind])}\n",
    "        \n",
    "\n",
    "            # generate wordcloud\n",
    "            wordcloud = WordCloud().generate_from_frequencies(frequencies)\n",
    "        \n",
    "           \n",
    "            ax[k].imshow(wordcloud, interpolation='bilinear')\n",
    "            ax[k].axis(\"off\")\n",
    "        \n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we plot the alpha vs. number of clusters for the Algorithm (question c):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(alpha_values, K_plus)\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"number of clusters\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
